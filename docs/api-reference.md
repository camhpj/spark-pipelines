# API reference (curated)

This page lists the primary public entrypoints. It is intentionally curated
instead of autogenerated.

## CLI entrypoints

- `spark-preprocessor compile --pipeline <path> --out <dir>`
- `spark-preprocessor render-sql --pipeline <path> --out <dir>`
- `spark-preprocessor test --pipeline <path> --project <dir>`
- `spark-preprocessor scaffold --mapping <path> --out <dir>`

## Runtime entrypoint (Databricks)

- `spark_preprocessor.runtime.apply_pipeline:main`
  - Arguments: `--pipeline`, `--project`, optional `--environment`.

## Compiler

- `spark_preprocessor.compiler.compile_pipeline(pipeline_path, out_dir) -> CompileReport`
  - Compiles a pipeline YAML into SQLMesh assets and artifacts.

## Schema loading

- `spark_preprocessor.schema.load_pipeline_document(path) -> PipelineDocument`
- `spark_preprocessor.schema.load_mapping_spec(path) -> MappingSpec`

## Features

- `spark_preprocessor.features.register_feature(feature)`
- `spark_preprocessor.features.get_feature(key)`
- `spark_preprocessor.features.list_features()`

Core types:

- `FeatureMetadata`, `FeatureParamSpec`, `FeatureRequirement`, `ColumnSpec`
- `FeatureAssets`, `SqlmeshModelSpec`, `JoinModelSpec`, `SqlmeshTestSpec`
- `BuildContext`

## Semantic contract

- `spark_preprocessor.semantic_contract.default_semantic_contract()`

## Errors

All custom errors derive from `SparkPreprocessorError`:

- `ConfigurationError`
- `ValidationError`
- `FeatureNotFoundError`
- `CompileError`
