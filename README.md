# spark-preprocessor

## Overview

`spark-preprocessor` is a standalone Python library that compiles a pipeline YAML into a
SQLMesh project, a rendered SQL model, and an optional Databricks profiling notebook.
It is a pure-Python compiler: Spark is only required when you run the generated artifacts
in Databricks.

Primary libraries used:
- **SQLMesh** for model/project generation and runtime execution.
- **SQLGlot** for Spark SQL parsing/validation.
- **Pydantic** for strict pipeline/mapping schema validation.
- **ydata-profiling** for Spark-based profiling in the generated notebook.
- **structlog** for structured logging.

## Documentation

Hand-written guides live in `docs/`:

- `docs/README.md` (index)
- `docs/overview.md` (what it does)
- `docs/architecture.md` (compiler/runtime flow)
- `docs/pipeline.md` / `docs/mapping.md` / `docs/profiling.md` (YAML reference)
- `docs/features.md` (feature authoring)
- `docs/runtime.md` (Databricks execution)
- `docs/testing.md` (test expectations)
- `docs/troubleshooting.md` (common errors)
- `docs/api-reference.md` (curated API)

## Technical Implementation

High-level flow:
1. **Parse & validate** a single YAML document (`mapping`, `pipeline`, `features`, optional `profiling`) using Pydantic.
2. **Validate** against the `SemanticContract` (required columns + naming rules).
3. **Generate semantic views** in SQLMesh (`semantic.<entity>` / `semantic.reference__<name>`).
4. **Build feature assets** from the registry (select expressions + optional join models).
5. **Assemble final model** (spine + joins + feature columns), embed metadata comments, and render SQL.
6. **Emit artifacts** into a deterministic `out_dir` layout (SQLMesh config, models, rendered SQL, compile report, notebook).

Key implementation points:
- Feature registry auto-registers built-ins on import.
- Column naming collisions are controlled by the naming policy (`fail` or `auto_prefix`).
- Compilation wipes `out_dir` and regenerates all artifacts idempotently.
- Runtime entrypoint (`spark_preprocessor.runtime.apply_pipeline:main`) initializes a SQLMesh `Context`, plans, and applies.

## Usage

### Local Example (DuckDB)

The `example/` directory demonstrates a minimal pipeline with a dummy dataset
generated by **Faker** (see `example/data/patients.csv` or regenerate it).

```bash
task install
uv run python example/generate_data.py --rows 100
uv run spark-preprocessor compile --pipeline example/pipeline.yaml --out example/out
uv run python example/run_duckdb.py
```

Outputs will be written to `example/out/`, and the DuckDB script will print sample rows
from `semantic.enriched_example`.

### Databricks Runtime

This project is designed to execute **inside** Databricks only. It does not connect to
Databricks from an external environment. The runtime uses the active Spark session
available on the cluster.

In Databricks, configure a job task to run the wheel entrypoint:

```
spark_preprocessor.runtime.apply_pipeline:main
```

Pass `--pipeline <path>` and `--project <dir>` pointing at the compiled artifacts.
Both paths should be accessible from the Databricks job (DBFS or workspace files).

Typical flow:
1. Build the wheel and upload it to the cluster/job.
2. Compile the pipeline to a SQLMesh project (`spark-preprocessor compile`) and
   make the compiled `out_dir` available to the job.
3. Run the job task with the entrypoint above to plan/apply using the in-cluster Spark session.

### Optional Extras

- `duckdb`: local execution smoke tests (DuckDB + PyArrow)
- `databricks`: Databricks SQL connector + PySpark (useful for local tooling; clusters already include Spark)

## Development Setup

This project uses **`uv`** for Python dependency management, **`Task`** for common dev commands, and **Docker** for containerized workflows (where applicable).

### Prerequisites

Make sure the following tools are installed on your system:

#### 1. `uv` (Python package & environment manager)

Install `uv` following the official instructions:

* macOS / Linux:

  ```bash
  curl -LsSf https://astral.sh/uv/install.sh | sh
  ```
* Windows (PowerShell):

  ```powershell
  irm https://astral.sh/uv/install.ps1 | iex
  ```

Verify:

```bash
uv --version
```

---

#### 2. Task (Taskfile runner)

This project standardizes developer and CI commands via **Taskfile**.

Install Task:

* macOS:

  ```bash
  brew install go-task/tap/go-task
  ```
* Linux:

  ```bash
  sh -c "$(curl --location https://taskfile.dev/install.sh)" -- -d -b ~/.local/bin
  ```
* Windows:

  ```powershell
  choco install go-task
  ```

Verify:

```bash
task --version
```

---

#### 3. Docker (optional, for dockerized projects)

If you are working on a dockerized variant of this project, install:

* **Docker Desktop** (macOS / Windows)
* **Docker Engine + Compose plugin** (Linux)

Verify:

```bash
docker --version
docker compose version
```

---

## Initial Project Setup

Once prerequisites are installed:

```bash
task install
```

This will:

* Create / sync the virtual environment
* Install all Python dependencies via `uv`

---

## Common Development Commands

```bash
task dev          # format, lint, and test
task lint         # ruff checks
task fmt          # apply formatting
task test         # run pytest
task ci           # CI-equivalent checks (no formatting writes)
```

To see all available tasks:

```bash
task --list
```

---

## Versioning w/ Commitizen
This project is versioned using commitizen with conventional commits.

Basic usage:
```bash
# get current project version
task version  

# commit changes w/ conventional commit
git add -A
git commit -m "feat: cool new feature"

# bump project version
task version:bump

# push w/ tags
git push origin HEAD --follow-tags
```

## Docker Workflow (if applicable)

```bash
task docker:up          # start containers using existing images
task docker:up:build    # rebuild images, then start containers
task docker:down        # stop containers
task docker:down:v      # stop containers and remove volumes (destructive)
task docker:logs        # tail container logs
```

---

## VS Code Setup (Recommended)

### Install Extensions

* **`ty`** (Python language server / tooling)
* **Python** (Microsoft) — optional, but common for debugging and tooling

### Disable the Built-in Python Language Server

To avoid running **two Python language servers simultaneously**, add the following to your VS Code `settings.json`:

```json
{
  "python.languageServer": "None"
}
```

This ensures `ty` is the sole active language server.

---

## Notes & Conventions

* Python >= 3.13 and a `src/` layout are required
* Formatting and linting are handled **exclusively by Ruff** (run via Taskfile)
* Type checking is handled by **ty** (run via Taskfile)
* Tests are mandatory; add tests with new features/behavior
* `from __future__ import annotations` is not allowed
* Compilation wipes `out_dir` and regenerates all artifacts deterministically
* The library runs inside Databricks; no external Databricks connections are supported
* Test configuration (pytest flags, markers, etc.) lives in `pyproject.toml`
* Dependency changes should be committed via `uv` lock updates
* Docker volumes are **not** removed by default—use `docker:down:v` explicitly if needed
